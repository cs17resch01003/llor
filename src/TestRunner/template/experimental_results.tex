\section{Experimental Results}
\tableref{results} summarizes the results obtained from running \tool against the benchmark suite. The table categorizes the results into three categories based on the output of \verifier. The \hyperlink{racefree}{first category} includes all the programs for which \verifier concluded that there were no errors. For \hyperlink{racefree_changes}{$11$ programs} in this category, \tool recommended changes.

The \hyperlink{racedetected}{second category} includes the programs for which \verifier had identified data races. Out of the $147$ programs in this category, \tool was able to fix \hyperlink{repaired}{$107$ programs} correctly. \tool was unable to repair \hyperlink{repairerror}{$16$ programs}. \hyperlink{unsupported}{$15$ programs} were using OpenMP constructs like \EM{sections} and \EM{simd}. These programs are beyond the scope of \tool since barriers and ordered regions cannot fix data races in these programs. \tool clearly states that these programs are unsupported when provided as input. \hyperlink{timedout}{$9$ programs} timed out.

The \hyperlink{llovunsupported}{final category} includes the programs that are either unsupported by \verifier or failed compilation. There are $64$ programs in this category. \verifier threw compilation errors for \hyperlink{compileerror}{$10$ programs}. \verifier does not support the verification of OpenMP programs that use task-based parallelism. Since \tool depends on \verifier in the repair process, it could not generate any repair candidates for \hyperlink{llorunsupported}{$48$ programs}. \verifier either timed out or threw a runtime error for \hyperlink{verifyerror}{$6$ programs}.
 
\input{tables/results}

\subsection{Solver Comparison}

\tool offers two different ways of solving the clauses generated from the error traces. The default approach uses the minimal-hitting-set ($mhs$) strategy. The user can switch to the $MaxSAT$ strategy if they wish to. Out of the $415$ benchmarks, only $133$ benchmarks did not result in a timeout and needed the use of a solver strategy (either $mhs$ or $MaxSAT$). In $93$ of these benchmarks, the $mhs$ strategy performed better than the $MaxSAT$ strategy. \figref{solver_time} shows the comparison of these benchmarks.

\input{figures/solvertime}
